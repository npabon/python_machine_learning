{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom LINCS random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=20000, \n",
    "                                            n_features=36, \n",
    "                                            n_informative=10, \n",
    "                                            n_redundant=6, \n",
    "                                            n_repeated=0, \n",
    "                                            n_classes=2, \n",
    "                                            n_clusters_per_class=2, \n",
    "                                            weights=None, \n",
    "                                            flip_y=0.01, \n",
    "                                            class_sep=1.0, \n",
    "                                            hypercube=True, \n",
    "                                            shift=0.0, \n",
    "                                            scale=1.0, \n",
    "                                            shuffle=True, \n",
    "                                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 36)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to randomly remove data from X in order to simulate the missing data from our LINCS classification problem. Basically every sample has **4 metrics x 9 cell lines = 36 total features**. However, not all samples are tested in all cell lines, but we will say that have at minimum data from four cell lines. This should be made a variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first assign which cell lines each sample was tested in\n",
    "min_n_cells = 4\n",
    "max_n_cells = 9\n",
    "n_cells_ = np.random.randint(min_n_cells, max_n_cells, len(y))\n",
    "n_missing_cells_ = 9 - n_cells_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove features from each sample's missing cell lines\n",
    "X_df = pd.DataFrame(X)\n",
    "\n",
    "for index in range(len(X)):\n",
    "    n_missing_cells = n_missing_cells_[index]\n",
    "    \n",
    "    # randomly choose which cells lines are mising\n",
    "    missing_cell_lines = np.random.choice(np.arange(9),n_missing_cells, replace=False)\n",
    "    \n",
    "    # convert this to the missing feature indeces\n",
    "    missing_feature_idx = np.array([ 4*i + np.array([0, 1, 2, 3]) for i in missing_cell_lines ]).reshape(1,-1)[0]\n",
    "    \n",
    "    # remove the feature values\n",
    "    X_df.set_value(index, missing_feature_idx, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.209149</td>\n",
       "      <td>1.192170</td>\n",
       "      <td>-1.164384</td>\n",
       "      <td>0.484403</td>\n",
       "      <td>-0.456668</td>\n",
       "      <td>1.353219</td>\n",
       "      <td>-1.084165</td>\n",
       "      <td>-0.510960</td>\n",
       "      <td>-1.074072</td>\n",
       "      <td>-0.376052</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.528943</td>\n",
       "      <td>0.326389</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.308625</td>\n",
       "      <td>-0.476750</td>\n",
       "      <td>-1.852631</td>\n",
       "      <td>-0.192746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.724587</td>\n",
       "      <td>0.857552</td>\n",
       "      <td>-0.282390</td>\n",
       "      <td>-0.872162</td>\n",
       "      <td>-1.295439</td>\n",
       "      <td>-1.104565</td>\n",
       "      <td>1.008983</td>\n",
       "      <td>1.783967</td>\n",
       "      <td>0.331429</td>\n",
       "      <td>0.038724</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.679008</td>\n",
       "      <td>-0.605346</td>\n",
       "      <td>2.459989</td>\n",
       "      <td>-1.541449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.064542</td>\n",
       "      <td>1.273224</td>\n",
       "      <td>3.966204</td>\n",
       "      <td>0.601887</td>\n",
       "      <td>0.774822</td>\n",
       "      <td>-0.204536</td>\n",
       "      <td>0.246851</td>\n",
       "      <td>-1.001860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.064790</td>\n",
       "      <td>-0.591569</td>\n",
       "      <td>0.717058</td>\n",
       "      <td>-1.076956</td>\n",
       "      <td>2.132901</td>\n",
       "      <td>0.559781</td>\n",
       "      <td>-1.215091</td>\n",
       "      <td>0.669849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.123821</td>\n",
       "      <td>1.134245</td>\n",
       "      <td>-4.068682</td>\n",
       "      <td>-0.384080</td>\n",
       "      <td>1.600720</td>\n",
       "      <td>1.042394</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.302279</td>\n",
       "      <td>0.459751</td>\n",
       "      <td>-0.754988</td>\n",
       "      <td>0.204575</td>\n",
       "      <td>-2.347061</td>\n",
       "      <td>1.891265</td>\n",
       "      <td>-4.264726</td>\n",
       "      <td>-1.290770</td>\n",
       "      <td>0.073044</td>\n",
       "      <td>-0.258931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.244846</td>\n",
       "      <td>0.978361</td>\n",
       "      <td>-0.988202</td>\n",
       "      <td>1.456638</td>\n",
       "      <td>-0.067727</td>\n",
       "      <td>0.233124</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.776104</td>\n",
       "      <td>0.527975</td>\n",
       "      <td>-1.371177</td>\n",
       "      <td>0.927110</td>\n",
       "      <td>-1.722779</td>\n",
       "      <td>0.150167</td>\n",
       "      <td>-0.023199</td>\n",
       "      <td>-2.650051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  1.209149  1.192170 -1.164384  0.484403 -0.456668  1.353219 -1.084165   \n",
       "1 -0.724587  0.857552 -0.282390 -0.872162 -1.295439 -1.104565  1.008983   \n",
       "2  2.064542  1.273224  3.966204  0.601887  0.774822 -0.204536  0.246851   \n",
       "3       NaN       NaN       NaN       NaN  0.123821  1.134245 -4.068682   \n",
       "4       NaN       NaN       NaN       NaN -1.244846  0.978361 -0.988202   \n",
       "\n",
       "         7         8         9     ...           26        27        28  \\\n",
       "0 -0.510960 -1.074072 -0.376052    ...    -2.528943  0.326389       NaN   \n",
       "1  1.783967  0.331429  0.038724    ...          NaN       NaN       NaN   \n",
       "2 -1.001860       NaN       NaN    ...          NaN       NaN -1.064790   \n",
       "3 -0.384080  1.600720  1.042394    ...    -7.302279  0.459751 -0.754988   \n",
       "4  1.456638 -0.067727  0.233124    ...          NaN       NaN -0.776104   \n",
       "\n",
       "         29        30        31        32        33        34        35  \n",
       "0       NaN       NaN       NaN -0.308625 -0.476750 -1.852631 -0.192746  \n",
       "1       NaN       NaN       NaN  1.679008 -0.605346  2.459989 -1.541449  \n",
       "2 -0.591569  0.717058 -1.076956  2.132901  0.559781 -1.215091  0.669849  \n",
       "3  0.204575 -2.347061  1.891265 -4.264726 -1.290770  0.073044 -0.258931  \n",
       "4  0.527975 -1.371177  0.927110 -1.722779  0.150167 -0.023199 -2.650051  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have a dataset with missing values that mimic the missing data we have in our LINCS dataset. Now we have to construct our custom Random Forest implementation that elegantly handles the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 4. 6. ... 6. 8. 5.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_missing = X_df.values\n",
    "X_not_missing = ~np.isnan(X_missing)\n",
    "num_cells_not_missing = np.count_nonzero(X_not_missing, axis=1) / 4\n",
    "print(num_cells_not_missing)\n",
    "np.min(num_cells_not_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try a classic SKLEARN random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=-1, oob_score=False, random_state=1,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(criterion='gini',\n",
    "                                n_estimators=10,\n",
    "                                random_state=1,\n",
    "                                n_jobs=-1)\n",
    "forest.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[ tree.score(X,y) for tree in forest.estimators_ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 0., 0., ..., 1., 0., 0.]),\n",
       " array([0., 0., 0., ..., 1., 0., 0.]),\n",
       " array([0., 1., 0., ..., 1., 0., 0.]),\n",
       " array([0., 0., 0., ..., 1., 0., 0.]),\n",
       " array([0., 0., 0., ..., 1., 0., 0.]),\n",
       " array([0., 0., 0., ..., 1., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 1., 0., 0.]),\n",
       " array([0., 0., 0., ..., 1., 0., 0.]),\n",
       " array([0., 0., 0., ..., 1., 0., 1.])]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ tree.predict(X) for tree in forest.estimators_ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "LRF = LincsRandomForestClassifier(n_cells_per_forest=4)\n",
    "LRF.fit(X_missing, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import itertools\n",
    "\n",
    "class LincsRandomForestClassifier(object):\n",
    "    \n",
    "    \"WE ASSUME THE DATA IS GROUPED BY CELL LINE AND HAS 4 FEATURES PER CELL LINE\"\n",
    "   \n",
    "    def __init__(self, n_cells_per_forest, n_estimators_per_forest=10, max_depth=None, max_features=\"auto\", random_state=1):\n",
    "        self.n_cells_per_forest = n_cells_per_forest\n",
    "        self.n_estimators_per_forest = n_estimators_per_forest\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Train several random forests, each one on a different\n",
    "        subset of cells. Store forests in a dictionary called\n",
    "        self.forests.\n",
    "        '''\n",
    "        # make sure we have enough data to work with\n",
    "        min_num_cells = self.get_min_num_cells(X)\n",
    "        assert min_num_cells >= self.n_cells_per_forest, \"Too much missing data for n_cells_per_forest = %s. (Some samples only tested in %d cells)\" % \\\n",
    "                                                         (self.n_cells_per_forest, min_num_cells)\n",
    "        \n",
    "        # generate cell subsets for training\n",
    "        total_num_cells = int(X.shape[1] / 4) # THIS IS HARDCODED IN\n",
    "        cell_subsets = itertools.combinations(np.arange(total_num_cells), self.n_cells_per_forest)\n",
    "        \n",
    "        # initialize dictionary to hold the forests\n",
    "        self.forests = {}\n",
    "        \n",
    "        # train forest on each subset\n",
    "        for cell_subset in cell_subsets:\n",
    "            \n",
    "            # find samples that have complete data from the cell subset\n",
    "            cell_subset_idx = np.array([ 4*i + np.array([0, 1, 2, 3])for i in cell_subset ]).reshape(1,-1)[0].astype(int)\n",
    "            cell_subset_data = X[:,cell_subset_idx]\n",
    "            bad_sample_idx = np.isnan(cell_subset_data).any(axis=1)\n",
    "            good_samples = cell_subset_data[~bad_sample_idx]\n",
    "            good_labels = y[~bad_sample_idx]\n",
    "            \n",
    "            # train and store a RF classifier on this training subset\n",
    "            # print('Growing forest for cell subset: %s' % str(cell_subset))\n",
    "            forest = RandomForestClassifier(criterion='gini',\n",
    "                                            n_estimators=self.n_estimators_per_forest,\n",
    "                                            max_depth=self.max_depth,\n",
    "                                            max_features=self.max_features,\n",
    "                                            random_state=self.random_state,\n",
    "                                            n_jobs=-1)\n",
    "            forest.fit(good_samples, good_labels)\n",
    "            self.forests[cell_subset] = forest            \n",
    "\n",
    "        \n",
    "    def get_min_num_cells(self, X):\n",
    "        '''\n",
    "        Calculate the minimum number of cells any sample has data for\n",
    "        ASSUMES EACH CELL LINE HAS 4 FEATURES\n",
    "        '''\n",
    "        X_not_missing = ~np.isnan(X)\n",
    "        num_cells_not_missing = np.count_nonzero(X_not_missing, axis=1) / 4\n",
    "        min_num_cells = np.min(num_cells_not_missing)\n",
    "        return min_num_cells\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Return the class probabilities label OF ONE SINGLE SAMPLE FOR FUCKS SAKE\n",
    "        '''\n",
    "        # figure out which cell lines we have data for\n",
    "        non_nan_idx = np.where(np.isnan(X) == False)[0]\n",
    "        good_cells = (non_nan_idx[np.where(non_nan_idx/4%1 == 0)[0]] / 4).astype(int)\n",
    "        \n",
    "        # select appropriate forests and predict\n",
    "        cell_subsets = itertools.combinations(good_cells, self.n_cells_per_forest)\n",
    "        tree_predictions_ = []\n",
    "        for cell_subset in cell_subsets:\n",
    "            # extract appropriate data\n",
    "            cell_subset_idx = np.array([ 4*i + np.array([0, 1, 2, 3])for i in cell_subset ]).reshape(1,-1)[0].astype(int)\n",
    "            cell_subset_data = X[cell_subset_idx].reshape(1,-1) \n",
    "            # extract appropriate forest and make prediction\n",
    "            forest = self.forests[cell_subset]\n",
    "            tree_predictions = [ tree.predict(cell_subset_data) for tree in forest.estimators_ ]\n",
    "            tree_predictions_.append(tree_predictions)\n",
    "        \n",
    "        # majority vote of all the trees in all the forests\n",
    "        results = np.array(tree_predictions_).flatten()\n",
    "        proba = results.sum() / len(results)\n",
    "        return np.array([1.-proba, proba])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Return the predicted class label OF ONE SINGLE SAMPLE FOR FUCKS SAKE\n",
    "        '''\n",
    "        class_probabilities = self.predict_proba(X)\n",
    "        return np.argmax(class_probabilities)\n",
    "    \n",
    "    def predict_proba_(self, X):\n",
    "        '''\n",
    "        for a multidimentional X\n",
    "        '''\n",
    "        proba_ = np.array([ self.predict_proba(x) for x in X ])\n",
    "        return proba_\n",
    "    \n",
    "    def predict_(self, X):\n",
    "        '''\n",
    "        for a multidimentional X\n",
    "        '''\n",
    "        predicted_classes = np.array([ self.predict(x) for x in X ])\n",
    "        return predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
